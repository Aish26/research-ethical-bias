{"cells":[{"cell_type":"code","source":["!pip install pyspark\n","# -------------------------------------------------\n","from pyspark.sql import SparkSession\n","from pyspark import SparkConf, SparkContext\n","from datetime import datetime, date, timedelta\n","from dateutil import relativedelta\n","from pyspark.sql import SQLContext, Row\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","from pyspark.sql import DataFrame\n","from pyspark.sql.functions import *\n","from pyspark.sql.functions import to_timestamp, to_date\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import collect_list, collect_set, concat, first, array_distinct, col, size, expr\n","from pyspark.sql import DataFrame\n","import random\n","import pandas as pd\n","# --------------------------------------------------------------\n","spark = SparkSession\\\n","        .builder\\\n","        .getOrCreate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSI6T0nmY-2E","executionInfo":{"status":"ok","timestamp":1686202504968,"user_tz":420,"elapsed":57018,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}},"outputId":"e41ce33d-f9a4-43e6-8d7b-73d46be32783"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=0e9c0c72d0d59be1ae54fb2eb8ba7ec25c400f1c4ee778aa42a163e57caf5368\n","  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.0\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScAMKvnXLKLj","executionInfo":{"status":"ok","timestamp":1686202558618,"user_tz":420,"elapsed":53657,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}},"outputId":"e5619d3f-f762-4d52-8925-42a2a78e7ab0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"LHFI2PWDuNx-","executionInfo":{"status":"ok","timestamp":1686202558619,"user_tz":420,"elapsed":11,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"outputs":[],"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4uvXa5mIQrQ","executionInfo":{"status":"ok","timestamp":1686202583634,"user_tz":420,"elapsed":25025,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}},"outputId":"65442196-17b3-4cf1-eb57-980de71e585e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["+--------+-------+----------------+-------------------+--------------------+-----------+--------------------+\n","|   user1|  user2|transaction_type|           datetime|         description|is_business|            story_id|\n","+--------+-------+----------------+-------------------+--------------------+-----------+--------------------+\n","| 1218774|1528945|         payment|2015-11-27 10:48:19|                Uber|      false|5657c473cd03c9af2...|\n","| 5109483|4782303|         payment|2015-06-17 11:37:04|              Costco|      false|5580f9702b64f70ab...|\n","| 4322148|3392963|         payment|2015-06-19 07:05:31|        Sweaty balls|      false|55835ccb1a624b14a...|\n","|  469894|1333620|          charge|2016-06-03 23:34:13|                  üé•|      false|5751b185cd03c9af2...|\n","| 2960727|3442373|         payment|2016-05-29 23:23:42|                   ‚ö°|      false|574b178ecd03c9af2...|\n","| 3977544|2709470|         payment|2016-09-29 22:12:07|          Chipotlaid|      false|57ed2f4723e064eac...|\n","| 3766386|4209061|         payment|2016-05-20 10:31:15|     kitchen counter|      false|573e8503cd03c9af2...|\n","|  730075| 804466|         payment|2016-05-26 04:46:45|                Food|      false|57461d46cd03c9af2...|\n","| 5221751|4993533|         payment|2016-07-14 22:53:49|               Zaxby|      false|5787b58d23e064eac...|\n","| 6843582|7308338|         payment|2016-08-31 10:32:46|           Fan sucks|      false|57c64fdf23e064eac...|\n","| 5317324|3942984|         payment|2016-01-04 09:11:25|                  üë†|      false|5689c6bdcd03c9af2...|\n","| 1134661|1556430|         payment|2015-10-09 01:53:52|         Thanks babe|      false|5616bbc0cd03c9af2...|\n","| 4238868|4879587|         payment|2015-10-04 08:28:01|                  üç∫|      false|561080a1cd03c9af2...|\n","|11719500|8702716|         payment|2016-07-07 21:40:39|                   ‚õΩ|      false|577e69e723e064eac...|\n","| 3625798|5692302|         payment|2016-10-16 14:43:41|Hey man  it's bee...|      false|58032fad23e064eac...|\n","|  613908|3045405|          charge|2016-05-07 06:42:17|         Getaway car|      false|572d2bd9cd03c9af2...|\n","| 4682257|1870271|         payment|2016-02-24 09:14:12|     üîÆ gypsy things|      false|56cd03e4cd03c9af2...|\n","| 9414481|2869012|         payment|2016-04-09 09:19:46|                  üî¥|      false|570866c2cd03c9af2...|\n","|  241386|2580543|         payment|2015-05-17 06:00:19|           Furniture|      false|5557cc0407f81c33e...|\n","|  656477| 656214|          charge|2013-12-14 22:43:27|bed bath mostly b...|      false|52ac6e93d56b6bac5...|\n","+--------+-------+----------------+-------------------+--------------------+-----------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["sqlContext = SQLContext(spark)\n","\n","data_df = sqlContext.read.parquet(\"/content/drive/MyDrive/Big Data/VenmoSample.snappy.parquet\")\n","\n","data_df.show()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8erIpc8NAMjo","executionInfo":{"status":"ok","timestamp":1686205608019,"user_tz":420,"elapsed":1994505,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}},"outputId":"7adea7e6-ca22-417f-c4f9-161492d28481"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-6e9d3ee24a03>:13: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n","\n","\n","  for i, chunk in enumerate(pd.read_csv(filepath, delimiter='\\t', header=None, error_bad_lines=False, chunksize=chunksize)):\n","Skipping line 215018: expected 2 fields, saw 3\n","\n","Skipping line 272042: expected 2 fields, saw 3\n","Skipping line 295806: expected 2 fields, saw 3\n","\n","Skipping line 516611: expected 2 fields, saw 3\n","\n","<ipython-input-5-6e9d3ee24a03>:13: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n","\n","\n","  for i, chunk in enumerate(pd.read_csv(filepath, delimiter='\\t', header=None, error_bad_lines=False, chunksize=chunksize)):\n","Skipping line 26397: expected 2 fields, saw 3\n","\n","Skipping line 106563: expected 2 fields, saw 3\n","Skipping line 108494: expected 2 fields, saw 3\n","Skipping line 113144: expected 2 fields, saw 3\n","Skipping line 143961: expected 2 fields, saw 3\n","\n","Skipping line 167805: expected 2 fields, saw 3\n","Skipping line 189935: expected 2 fields, saw 3\n","\n","Skipping line 213813: expected 2 fields, saw 3\n","\n","Skipping line 251883: expected 2 fields, saw 3\n","Skipping line 290022: expected 2 fields, saw 3\n","Skipping line 291390: expected 2 fields, saw 3\n","\n","Skipping line 310073: expected 2 fields, saw 3\n","\n","Skipping line 510397: expected 2 fields, saw 3\n","Skipping line 526347: expected 2 fields, saw 3\n","Skipping line 532080: expected 2 fields, saw 3\n","\n","Skipping line 674734: expected 2 fields, saw 3\n","Skipping line 695020: expected 2 fields, saw 3\n","\n","Skipping line 701648: expected 2 fields, saw 3\n","Skipping line 732967: expected 2 fields, saw 3\n","\n","Skipping line 762785: expected 2 fields, saw 3\n","\n","Skipping line 944750: expected 2 fields, saw 3\n","\n","Skipping line 960700: expected 2 fields, saw 3\n","Skipping line 966433: expected 2 fields, saw 3\n","\n","Skipping line 1109087: expected 2 fields, saw 3\n","Skipping line 1129373: expected 2 fields, saw 3\n","Skipping line 1136001: expected 2 fields, saw 3\n","\n","Skipping line 1167320: expected 2 fields, saw 3\n","Skipping line 1197138: expected 2 fields, saw 3\n","\n","<ipython-input-5-6e9d3ee24a03>:13: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n","\n","\n","  for i, chunk in enumerate(pd.read_csv(filepath, delimiter='\\t', header=None, error_bad_lines=False, chunksize=chunksize)):\n","Skipping line 185019: expected 2 fields, saw 3\n","\n","Skipping line 218199: expected 2 fields, saw 3\n","\n","Skipping line 254268: expected 2 fields, saw 3\n","\n","Skipping line 315096: expected 2 fields, saw 3\n","Skipping line 346167: expected 2 fields, saw 3\n","\n","Skipping line 379754: expected 2 fields, saw 3\n","\n","Skipping line 412250: expected 2 fields, saw 3\n","Skipping line 444606: expected 2 fields, saw 3\n","\n","Skipping line 482933: expected 2 fields, saw 3\n","Skipping line 493742: expected 2 fields, saw 3\n","Skipping line 498056: expected 2 fields, saw 3\n","\n","Skipping line 502557: expected 2 fields, saw 3\n","Skipping line 548650: expected 2 fields, saw 3\n","\n","Skipping line 550550: expected 2 fields, saw 3\n","Skipping line 556984: expected 2 fields, saw 3\n","Skipping line 570826: expected 2 fields, saw 3\n","Skipping line 579796: expected 2 fields, saw 3\n","Skipping line 580112: expected 2 fields, saw 3\n","Skipping line 592616: expected 2 fields, saw 3\n","Skipping line 598891: expected 2 fields, saw 3\n","Skipping line 599789: expected 2 fields, saw 3\n","\n","Skipping line 604257: expected 2 fields, saw 3\n","Skipping line 605031: expected 2 fields, saw 3\n","Skipping line 605176: expected 2 fields, saw 3\n","Skipping line 605856: expected 2 fields, saw 3\n","Skipping line 608317: expected 2 fields, saw 3\n","Skipping line 609053: expected 2 fields, saw 3\n","Skipping line 609331: expected 2 fields, saw 3\n","Skipping line 610425: expected 2 fields, saw 3\n","Skipping line 621055: expected 2 fields, saw 3\n","Skipping line 623071: expected 2 fields, saw 3\n","Skipping line 624229: expected 2 fields, saw 3\n","Skipping line 638836: expected 2 fields, saw 3\n","Skipping line 639222: expected 2 fields, saw 3\n","Skipping line 639288: expected 2 fields, saw 3\n","Skipping line 639693: expected 2 fields, saw 3\n","Skipping line 646430: expected 2 fields, saw 3\n","Skipping line 646564: expected 2 fields, saw 3\n","Skipping line 647209: expected 2 fields, saw 3\n","\n","Skipping line 658598: expected 2 fields, saw 3\n","Skipping line 658641: expected 2 fields, saw 3\n","Skipping line 658809: expected 2 fields, saw 3\n","Skipping line 658964: expected 2 fields, saw 3\n","Skipping line 659170: expected 2 fields, saw 3\n","Skipping line 659238: expected 2 fields, saw 3\n","Skipping line 659401: expected 2 fields, saw 3\n","Skipping line 659903: expected 2 fields, saw 3\n","Skipping line 660841: expected 2 fields, saw 3\n","Skipping line 681623: expected 2 fields, saw 3\n","Skipping line 690946: expected 2 fields, saw 3\n","\n","Skipping line 703212: expected 2 fields, saw 3\n","Skipping line 706725: expected 2 fields, saw 3\n","Skipping line 722351: expected 2 fields, saw 3\n","Skipping line 722409: expected 2 fields, saw 3\n","Skipping line 722470: expected 2 fields, saw 3\n","Skipping line 723145: expected 2 fields, saw 3\n","Skipping line 724528: expected 2 fields, saw 3\n","Skipping line 726243: expected 2 fields, saw 3\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","import pandas as pd\n","import ast\n","\n","def convert_to_dict(data):\n","    try:\n","        return ast.literal_eval(data.replace(\"u'\", \"'\").replace(\"'\", '\"'))\n","    except (ValueError, SyntaxError):\n","        return {}\n","\n","def process_data(spark, filepath, chunksize=50000):\n","    dfs = []\n","    for i, chunk in enumerate(pd.read_csv(filepath, delimiter='\\t', header=None, error_bad_lines=False, chunksize=chunksize)):\n","        # Extract 'id' column before processing\n","        id_col = chunk[0].astype(int)\n","        \n","        # Convert data to dictionary and process\n","        chunk[1] = chunk[1].apply(convert_to_dict)\n","        chunk = chunk[chunk[1].apply(len) > 0]\n","\n","        # Normalize each dictionary in the series into a dataframe\n","        dfs_chunk = [pd.json_normalize(d) if len(d) > 0 else pd.DataFrame() for d in chunk[1]]\n","        chunk = pd.concat(dfs_chunk, ignore_index=True)\n","        \n","        # Re-add the 'id' column to the chunk\n","        chunk['id'] = id_col\n","\n","        # Convert pandas dataframe to spark dataframe\n","        spark_df = spark.createDataFrame(chunk)\n","\n","        dfs.append(spark_df)\n","\n","    return dfs\n","\n","# Initialize Spark session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Load Venmo data\n","venmo_df = spark.read.parquet(\"/content/drive/MyDrive/Big Data/VenmoSample.snappy.parquet\")\n","\n","# List of file paths\n","paths = ['/content/drive/MyDrive/Big Data/Data/face1results.csv', '/content/drive/MyDrive/Big Data/Data/face2results.csv', '/content/drive/MyDrive/Big Data/Data/face3results.csv']\n","         #'/content/drive/MyDrive/Big Data/Data/face4results.csv', '/content/drive/MyDrive/Big Data/Data/face5results.csv']\n","\n","face_dfs = []\n","for path in paths:\n","    face_dfs.extend(process_data(spark, path, chunksize=50000))\n","\n","# Now you have Venmo data in `venmo_df`, and Face data in `face_dfs` list\n"]},{"cell_type":"code","source":["# Combine all face data into a single DataFrame\n","from functools import reduce\n","from pyspark.sql import DataFrame\n","\n","def unionAll(*dfs):\n","    return reduce(DataFrame.unionAll, dfs)\n","\n","face_df = unionAll(*face_dfs)\n","\n","# Now join Venmo data with Face data\n","combined_df = venmo_df.join(face_df, venmo_df.user1 == face_df.id)\n","\n","# Now you have a combined DataFrame `combined_df`\n"],"metadata":{"id":"BNcan8drRmjl","executionInfo":{"status":"ok","timestamp":1686205610133,"user_tz":420,"elapsed":2114,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VBxOlSM0uskV","executionInfo":{"status":"ok","timestamp":1686205610136,"user_tz":420,"elapsed":1,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["shape = (combined_df.count(), len(combined_df.columns))\n","print(shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFMOOZ91Zwjj","executionInfo":{"status":"ok","timestamp":1686205676193,"user_tz":420,"elapsed":66059,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}},"outputId":"4e096d61-01bc-4ca0-d1b9-7ed3e7e674ec"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(185988, 36)\n"]}]},{"cell_type":"code","source":["num_unique_users1 = combined_df.select(\"user1\").distinct().count()\n","print(num_unique_users1)\n"],"metadata":{"id":"xJ6z6_b6avJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_df.show()"],"metadata":{"id":"5IaBuzzoKCPn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_df.createOrReplaceTempView(\"df_temp\")\n","demographic_venmo = spark.sql(\"\"\"\n","    SELECT user1, `attribute.gender.confidence`, `attribute.gender.value` AS gender,\n","           `attribute.age.value` AS age,\n","           `attribute.race.confidence`, `attribute.race.value` as race\n","    FROM df_temp\n","\"\"\")\n","\n","demographic_venmo.show()"],"metadata":{"id":"PL1WVjnlfvMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"O0iRyLajNa85","executionInfo":{"status":"ok","timestamp":1686205857068,"user_tz":420,"elapsed":2,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Group by \"race\" column and count occurrences\n","race_counts = demographic_venmo.groupBy(\"race\").count()\n","\n","# Display the category counts\n","race_counts.show()"],"metadata":{"id":"8JQG4R0Rmrqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Group by \"race\" column and count occurrences\n","gender_counts = demographic_venmo.groupBy(\"gender\").count()\n","\n","# Display the category counts\n","gender_counts.show()"],"metadata":{"id":"3P1fiAJZm4D3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, OneHotEncoder\n","from pyspark.ml import Pipeline\n","\n","# Assuming you have already loaded the data into a PySpark DataFrame named 'data'\n","\n","# Define the columns to be one-hot encoded\n","categorical_columns = [\"race\", \"gender\"]\n","\n","# Create a list to store the stages of the pipeline\n","stages = []\n","\n","# Iterate over the categorical columns and perform string indexing and one-hot encoding\n","for column in categorical_columns:\n","    # StringIndexer: Converts categorical values to numerical values\n","    indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\")\n","    \n","    # OneHotEncoder: Performs one-hot encoding on the indexed values\n","    encoder = OneHotEncoder(inputCols=[indexer.getOutputCol()], outputCols=[column + \"_encoded\"])\n","    \n","    # Add the stages to the pipeline\n","    stages += [indexer, encoder]\n","\n","# Create a pipeline with all the stages\n","pipeline = Pipeline(stages=stages)\n","\n","# Fit the pipeline to the data\n","pipeline_model = pipeline.fit(demographic_venmo)\n","\n","# Transform the data using the pipeline\n","transformed_data = pipeline_model.transform(demographic_venmo)\n","\n","# Show the transformed data\n","transformed_data.show()\n"],"metadata":{"id":"nDFNWbFynBuw","executionInfo":{"status":"aborted","timestamp":1686206076358,"user_tz":420,"elapsed":1,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop the 'gender' and 'race' columns\n","transformed_data = transformed_data.drop(\"gender\", \"race\")\n","\n","# Show the updated DataFrame\n","transformed_data.show()"],"metadata":{"id":"m6LilRgPt8eg","executionInfo":{"status":"aborted","timestamp":1686206076360,"user_tz":420,"elapsed":3,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming you have the DataFrame named 'transformed_data'\n","\n","# Rename the column\n","transformed_data = transformed_data.withColumnRenamed(\"attribute.gender.confidence\", \"attribute_gender_confidence\")\n","\n","# Show the updated DataFrame\n","transformed_data.show()\n"],"metadata":{"id":"oto7Y_9Tv6_d","executionInfo":{"status":"aborted","timestamp":1686206076361,"user_tz":420,"elapsed":4,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming you have the DataFrame named 'transformed_data'\n","\n","# Rename the column\n","transformed_data = transformed_data.withColumnRenamed(\"attribute.race.confidence\", \"attribute_race_confidence\")\n","\n","# Show the updated DataFrame\n","transformed_data.show()"],"metadata":{"id":"_WT8PWf3wAyj","executionInfo":{"status":"aborted","timestamp":1686206076362,"user_tz":420,"elapsed":5,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NNQ4dWakVnJr","executionInfo":{"status":"aborted","timestamp":1686206076363,"user_tz":420,"elapsed":6,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spending_venmo = spark.read.parquet(\"/content/drive/MyDrive/Big Data/Data/spending_behavior_with_y.parquet\")"],"metadata":{"id":"PAuKLvOvdNDQ","executionInfo":{"status":"aborted","timestamp":1686206076364,"user_tz":420,"elapsed":7,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spending_venmo.createOrReplaceTempView(\"spending_venmo\")\n","\n","spending_venmo.show(2)"],"metadata":{"id":"iwQQGfp7fTJ2","executionInfo":{"status":"aborted","timestamp":1686206076365,"user_tz":420,"elapsed":8,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_variable = spark.sql(\"\"\"\n","    SELECT user1, y\n","    FROM spending_venmo\n","\"\"\")\n","\n","target_variable.show()"],"metadata":{"id":"-PF5Yh8tfjwr","executionInfo":{"status":"aborted","timestamp":1686206076366,"user_tz":420,"elapsed":9,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["joined_df = demographic_venmo.join(target_variable, demographic_venmo.user1 == target_variable.user1, \"left\")"],"metadata":{"id":"KjL1UAlJfxEz","executionInfo":{"status":"aborted","timestamp":1686206076367,"user_tz":420,"elapsed":10,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Assuming you have already loaded the DataFrame as 'df'\n","\n","# Drop duplicates based on all columns\n","joined_df = joined_df.dropDuplicates()\n","\n","# Show the updated DataFrame\n","joined_df.show()\n"],"metadata":{"id":"-3b7oM_xgsy5","executionInfo":{"status":"aborted","timestamp":1686206076369,"user_tz":420,"elapsed":12,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["joined_df.createOrReplaceTempView(\"behaviorial_with_y\")\n","\n","joined_df.show(2)"],"metadata":{"id":"20-55ub0gUwg","executionInfo":{"status":"aborted","timestamp":1686206076370,"user_tz":420,"elapsed":13,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import isnan, isnull\n","\n","# Check for null or NaN values in the target column\n","null_count = joined_df.filter(isnull(\"y\") | isnan(\"y\")).count()\n","\n","null_count"],"metadata":{"id":"_FRpVdcMhHAR","executionInfo":{"status":"aborted","timestamp":1686206076373,"user_tz":420,"elapsed":16,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pandas_df = joined_df.toPandas()\n"],"metadata":{"id":"LjiLWZacTReP","executionInfo":{"status":"aborted","timestamp":1686206076382,"user_tz":420,"elapsed":25,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pandas_df.head()"],"metadata":{"id":"txsS7UNoTUqe","executionInfo":{"status":"aborted","timestamp":1686206076386,"user_tz":420,"elapsed":29,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pandas_df.to_csv('/content/drive/MyDrive/Big Data/Data/behavioural_with_y_all.csv', index=False)\n"],"metadata":{"id":"55GiRSbZTjsn","executionInfo":{"status":"aborted","timestamp":1686206076388,"user_tz":420,"elapsed":31,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming you have a DataFrame named \"matchedData\"\n","\n","# Specify the output path for the Parquet file\n","output_path = \"/content/drive/MyDrive/Big Data/Data/face_all_joined_df.parquet\"\n","\n","from pyspark.sql.functions import col\n","\n","# Rename the existing column\n","df = joined_df.withColumnRenamed('user1', 'new_column_name')\n","\n","# Save the DataFrame as a Parquet file\n","joined_df.write.parquet(output_path)\n"],"metadata":{"id":"7-doeKPWPLxn","executionInfo":{"status":"aborted","timestamp":1686206076389,"user_tz":420,"elapsed":32,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["joined_df.show(2)"],"metadata":{"id":"uya_flxFaJN8","executionInfo":{"status":"aborted","timestamp":1686206076390,"user_tz":420,"elapsed":33,"user":{"displayName":"Priyanka Murugan","userId":"18383082567283970677"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1YoYoqEJT4AxXKr66UU6hi83wHj-GDLGf","timestamp":1684191592233}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}